{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "logger = logging.getLogger('sequence_tagger_bert')\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "logger.handlers = []\n",
    "\n",
    "fhandler = logging.handlers.TimedRotatingFileHandler(filename='logs.txt', when='midnight')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla V100-DGXS-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda')\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "for i in range(n_gpu):\n",
    "    print(torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = '../workdir/cache'\n",
    "BATCH_SIZE = 16\n",
    "#BATCH_SIZE = 8\n",
    "PRED_BATCH_SIZE = 1000\n",
    "MAX_LEN = 128\n",
    "MAX_N_EPOCHS = 10\n",
    "#MAX_N_EPOCHS = 100\n",
    "#MAX_N_EPOCHS = 50\n",
    "#MAX_N_EPOCHS = 10\n",
    "REDUCE_ON_PLATEAU = False\n",
    "WEIGHT_DECAY = 0.01\n",
    "LEARNING_RATE = 3e-6\n",
    "#LEARNING_RATE = 1e-5\n",
    "#LEARNING_RATE = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: pip3: not found\n"
     ]
    }
   ],
   "source": [
    "!pip3 install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANG=C.UTF-8\n",
      "LANGUAGE=\n",
      "LC_CTYPE=\"C.UTF-8\"\n",
      "LC_NUMERIC=\"C.UTF-8\"\n",
      "LC_TIME=\"C.UTF-8\"\n",
      "LC_COLLATE=\"C.UTF-8\"\n",
      "LC_MONETARY=\"C.UTF-8\"\n",
      "LC_MESSAGES=\"C.UTF-8\"\n",
      "LC_PAPER=\"C.UTF-8\"\n",
      "LC_NAME=\"C.UTF-8\"\n",
      "LC_ADDRESS=\"C.UTF-8\"\n",
      "LC_TELEPHONE=\"C.UTF-8\"\n",
      "LC_MEASUREMENT=\"C.UTF-8\"\n",
      "LC_IDENTIFICATION=\"C.UTF-8\"\n",
      "LC_ALL=\n"
     ]
    }
   ],
   "source": [
    "!locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LC_ALL=en_US.UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0610 23:45:15.415585 140316096190208 file_utils.py:39] PyTorch version 1.3.1 available.\n",
      "/home/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.\n",
      "Warning:  apex was installed without --cuda_ext. Fused syncbn kernels will be unavailable.  Python fallbacks will be used instead.\n",
      "Warning:  apex was installed without --cuda_ext.  FusedAdam will be unavailable.\n",
      "Warning:  apex was installed without --cuda_ext.  FusedLayerNorm will be unavailable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0610 23:45:17.491529 140316096190208 textcleaner.py:37] 'pattern' package not found; tag filters are not available for English\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-10 23:45:17,571 Reading data from /workspace/bert_sequence_tagger/src/data/NER/Varvara_v3\n",
      "2020-06-10 23:45:17,572 Train: /home/vika/targer/data/JUNE/train_pred_full_positive.tsv\n",
      "2020-06-10 23:45:17,572 Dev: /home/vika/targer/data/JUNE/dev_pred_full_positive.tsv\n",
      "2020-06-10 23:45:17,573 Test: /home/vika/targer/data/JUNE/dev_pred_full_positive.tsv\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode bytes in position 1692-1693: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-839d3a2684d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                       \u001b[0mtrain_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/home/vika/targer/data/JUNE/train_pred_full_positive.tsv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                       \u001b[0mtest_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/home/vika/targer/data/JUNE/dev_pred_full_positive.tsv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                       dev_file='/home/vika/targer/data/JUNE/dev_pred_full_positive.tsv')\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobtain_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/flair/datasets/sequence_labeling.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_folder, column_format, train_file, test_file, dev_file, tag_to_bioes, column_delimiter, comment_symbol, encoding, document_separator_token, skip_first_line, in_memory)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0min_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mdocument_separator_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocument_separator_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mskip_first_line\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_first_line\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         )\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/flair/datasets/sequence_labeling.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_to_column_file, column_name_map, tag_to_bioes, column_delimiter, comment_symbol, in_memory, document_separator_token, encoding, skip_first_line)\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0mposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode bytes in position 1692-1693: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "\n",
    "data_folder = '/workspace/bert_sequence_tagger/src/data/NER/Varvara_v3'\n",
    "corpus = ColumnCorpus(data_folder, \n",
    "                      {0 : 'text', 1 : 'tag'},\n",
    "                      train_file='/home/vika/targer/data/JUNE/train_pred_full_positive.tsv',\n",
    "                      test_file='/home/vika/targer/data/JUNE/dev_pred_full_positive.tsv',\n",
    "                      dev_file='/home/vika/targer/data/JUNE/dev_pred_full_positive.tsv')\n",
    "\n",
    "print(corpus.obtain_statistics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C.UTF-8\n"
     ]
    }
   ],
   "source": [
    "!printenv LANG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = corpus.make_tag_dictionary(tag_type = 'tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'<unk>',\n",
       " b'O',\n",
       " b'B-OBJ',\n",
       " b'B-PREDFULL',\n",
       " b'I-PREDFULL',\n",
       " b'I-OBJ',\n",
       " b'<START>',\n",
       " b'<STOP>']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.idx2item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0610 23:28:15.503033 140016840734464 modeling_bert.py:226] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "I0610 23:28:15.510476 140016840734464 modeling_xlnet.py:339] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "I0610 23:30:24.735812 140016840734464 file_utils.py:234] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt not found in cache, downloading to /tmp/tmpi_n5u6hv\n",
      "E0610 23:32:35.806068 140016840734464 tokenization_utils.py:289] Couldn't reach server to download vocabulary.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-302790f2c135>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbpe_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-cased'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0midx2tag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag2idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_bert_tag_dict_from_flair_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBertForTokenClassificationCustom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-cased'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag2idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "from bert_sequence_tagger import SequenceTaggerBert, BertForTokenClassificationCustom\n",
    "from pytorch_transformers import BertTokenizer, BertForTokenClassification\n",
    "import torch.nn as nn\n",
    "\n",
    "from bert_sequence_tagger.bert_utils import make_bert_tag_dict_from_flair_corpus\n",
    "\n",
    "\n",
    "bpe_tokenizer = BertTokenizer.from_pretrained('bert-base-cased', cache_dir=None, do_lower_case=False)\n",
    "\n",
    "idx2tag, tag2idx = make_bert_tag_dict_from_flair_corpus(corpus)\n",
    "\n",
    "model = nn.DataParallel(BertForTokenClassificationCustom.from_pretrained('bert-base-cased', cache_dir=None, num_labels=len(tag2idx))).cuda()\n",
    "#model = BertForTokenClassification.from_pretrained('bert-base-cased', cache_dir=CACHE_DIR, num_labels=len(tag2idx)).cuda()\n",
    "\n",
    "seq_tagger = SequenceTaggerBert(bert_model=model, bpe_tokenizer=bpe_tokenizer, idx2tag=idx2tag, tag2idx=tag2idx, max_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pytorch_transformers.tokenization_bert.BertTokenizer at 0x7f6250ad0390>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:18:43,851 - sequence_tagger_bert - INFO - Entity-level f1: 0.032533889468196034\n",
      "2019-11-20 00:18:43,852 - sequence_tagger_bert - INFO - Token-level f1: 0.06207475067573866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:19:36,479 - sequence_tagger_bert - INFO - Train loss: 0.3619607299953312\n",
      "2019-11-20 00:19:39,953 - sequence_tagger_bert - INFO - Validation loss: 0.3088280222401386\n",
      "2019-11-20 00:19:39,954 - sequence_tagger_bert - INFO - Validation metrics: (0.3985239852398525,)\n",
      "2019-11-20 00:19:39,973 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   5%|▌         | 1/20 [00:56<17:44, 56.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:20:32,119 - sequence_tagger_bert - INFO - Train loss: 0.08740380431174652\n",
      "2019-11-20 00:20:34,944 - sequence_tagger_bert - INFO - Validation loss: 0.39300361003090695\n",
      "2019-11-20 00:20:34,945 - sequence_tagger_bert - INFO - Validation metrics: (0.4347826086956522,)\n",
      "2019-11-20 00:20:34,964 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|█         | 2/20 [01:51<16:43, 55.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:21:26,956 - sequence_tagger_bert - INFO - Train loss: 0.06447363679309363\n",
      "2019-11-20 00:21:29,632 - sequence_tagger_bert - INFO - Validation loss: 0.42421448148968743\n",
      "2019-11-20 00:21:29,633 - sequence_tagger_bert - INFO - Validation metrics: (0.4388078630310716,)\n",
      "2019-11-20 00:21:29,650 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  15%|█▌        | 3/20 [02:45<15:42, 55.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:22:22,670 - sequence_tagger_bert - INFO - Train loss: 0.05221280366992699\n",
      "2019-11-20 00:22:25,633 - sequence_tagger_bert - INFO - Validation loss: 0.43800021946521067\n",
      "2019-11-20 00:22:25,635 - sequence_tagger_bert - INFO - Validation metrics: (0.44360428481411474,)\n",
      "2019-11-20 00:22:25,659 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 4/20 [03:41<14:49, 55.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:23:17,594 - sequence_tagger_bert - INFO - Train loss: 0.04596274611902992\n",
      "2019-11-20 00:23:20,634 - sequence_tagger_bert - INFO - Validation loss: 0.4470692996344552\n",
      "2019-11-20 00:23:20,635 - sequence_tagger_bert - INFO - Validation metrics: (0.45316455696202534,)\n",
      "2019-11-20 00:23:20,654 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  25%|██▌       | 5/20 [04:36<13:51, 55.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:24:11,892 - sequence_tagger_bert - INFO - Train loss: 0.04020843446532917\n",
      "2019-11-20 00:24:14,878 - sequence_tagger_bert - INFO - Validation loss: 0.48331384674259803\n",
      "2019-11-20 00:24:14,879 - sequence_tagger_bert - INFO - Validation metrics: (0.45218492716909436,)\n",
      "2019-11-20 00:24:14,880 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|███       | 6/20 [05:30<12:50, 55.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:25:06,428 - sequence_tagger_bert - INFO - Train loss: 0.0358084871920503\n",
      "2019-11-20 00:25:09,618 - sequence_tagger_bert - INFO - Validation loss: 0.48216514689166373\n",
      "2019-11-20 00:25:09,619 - sequence_tagger_bert - INFO - Validation metrics: (0.4474829086389062,)\n",
      "2019-11-20 00:25:09,621 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  35%|███▌      | 7/20 [06:25<11:54, 54.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:26:01,430 - sequence_tagger_bert - INFO - Train loss: 0.0332889553392306\n",
      "2019-11-20 00:26:04,259 - sequence_tagger_bert - INFO - Validation loss: 0.5220233976909119\n",
      "2019-11-20 00:26:04,261 - sequence_tagger_bert - INFO - Validation metrics: (0.46222222222222226,)\n",
      "2019-11-20 00:26:04,281 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 8/20 [07:20<10:58, 54.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:26:57,505 - sequence_tagger_bert - INFO - Train loss: 0.03205127487421737\n",
      "2019-11-20 00:27:00,557 - sequence_tagger_bert - INFO - Validation loss: 0.5088469949096623\n",
      "2019-11-20 00:27:00,558 - sequence_tagger_bert - INFO - Validation metrics: (0.46134347275031684,)\n",
      "2019-11-20 00:27:00,559 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  45%|████▌     | 9/20 [08:16<10:08, 55.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:27:52,452 - sequence_tagger_bert - INFO - Train loss: 0.029160591297444294\n",
      "2019-11-20 00:27:55,382 - sequence_tagger_bert - INFO - Validation loss: 0.48546486625039\n",
      "2019-11-20 00:27:55,383 - sequence_tagger_bert - INFO - Validation metrics: (0.46892307692307694,)\n",
      "2019-11-20 00:27:55,403 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 10/20 [09:11<09:11, 55.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:28:47,639 - sequence_tagger_bert - INFO - Train loss: 0.026940659996327738\n",
      "2019-11-20 00:28:50,424 - sequence_tagger_bert - INFO - Validation loss: 0.5129956489360732\n",
      "2019-11-20 00:28:50,425 - sequence_tagger_bert - INFO - Validation metrics: (0.47160493827160493,)\n",
      "2019-11-20 00:28:50,444 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  55%|█████▌    | 11/20 [10:06<08:16, 55.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:29:41,321 - sequence_tagger_bert - INFO - Train loss: 0.025348623533424763\n",
      "2019-11-20 00:29:44,479 - sequence_tagger_bert - INFO - Validation loss: 0.5219396179905389\n",
      "2019-11-20 00:29:44,480 - sequence_tagger_bert - INFO - Validation metrics: (0.471100062150404,)\n",
      "2019-11-20 00:29:44,482 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 12/20 [11:00<07:18, 54.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:30:36,152 - sequence_tagger_bert - INFO - Train loss: 0.02420229448667764\n",
      "2019-11-20 00:30:38,889 - sequence_tagger_bert - INFO - Validation loss: 0.5160302322176171\n",
      "2019-11-20 00:30:38,890 - sequence_tagger_bert - INFO - Validation metrics: (0.47095179233621753,)\n",
      "2019-11-20 00:30:38,891 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  65%|██████▌   | 13/20 [11:54<06:22, 54.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:31:31,216 - sequence_tagger_bert - INFO - Train loss: 0.0222365621215809\n",
      "2019-11-20 00:31:33,956 - sequence_tagger_bert - INFO - Validation loss: 0.5339932873924603\n",
      "2019-11-20 00:31:33,957 - sequence_tagger_bert - INFO - Validation metrics: (0.47051520794537555,)\n",
      "2019-11-20 00:31:33,958 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|███████   | 14/20 [12:50<05:28, 54.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:32:25,028 - sequence_tagger_bert - INFO - Train loss: 0.02142074331414414\n",
      "2019-11-20 00:32:28,010 - sequence_tagger_bert - INFO - Validation loss: 0.5332436034365035\n",
      "2019-11-20 00:32:28,011 - sequence_tagger_bert - INFO - Validation metrics: (0.4747225647348952,)\n",
      "2019-11-20 00:32:28,029 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75%|███████▌  | 15/20 [13:44<04:32, 54.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:33:19,202 - sequence_tagger_bert - INFO - Train loss: 0.0203145507642963\n",
      "2019-11-20 00:33:22,066 - sequence_tagger_bert - INFO - Validation loss: 0.5433915181187685\n",
      "2019-11-20 00:33:22,067 - sequence_tagger_bert - INFO - Validation metrics: (0.47211895910780666,)\n",
      "2019-11-20 00:33:22,067 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████  | 16/20 [14:38<03:37, 54.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:34:12,922 - sequence_tagger_bert - INFO - Train loss: 0.01986186942214134\n",
      "2019-11-20 00:34:15,760 - sequence_tagger_bert - INFO - Validation loss: 0.5410953026049111\n",
      "2019-11-20 00:34:15,761 - sequence_tagger_bert - INFO - Validation metrics: (0.4740740740740741,)\n",
      "2019-11-20 00:34:15,761 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  85%|████████▌ | 17/20 [15:31<02:42, 54.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:35:06,510 - sequence_tagger_bert - INFO - Train loss: 0.01940659679341668\n",
      "2019-11-20 00:35:09,299 - sequence_tagger_bert - INFO - Validation loss: 0.5366039614920027\n",
      "2019-11-20 00:35:09,300 - sequence_tagger_bert - INFO - Validation metrics: (0.4763076923076923,)\n",
      "2019-11-20 00:35:09,324 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  90%|█████████ | 18/20 [16:25<01:48, 54.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:36:01,028 - sequence_tagger_bert - INFO - Train loss: 0.018303427412839874\n",
      "2019-11-20 00:36:03,860 - sequence_tagger_bert - INFO - Validation loss: 0.5431420835098479\n",
      "2019-11-20 00:36:03,991 - sequence_tagger_bert - INFO - Validation metrics: (0.47466007416563666,)\n",
      "2019-11-20 00:36:03,992 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  95%|█████████▌| 19/20 [17:20<00:54, 54.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:36:54,500 - sequence_tagger_bert - INFO - Train loss: 0.018760964634991554\n",
      "2019-11-20 00:36:57,514 - sequence_tagger_bert - INFO - Validation loss: 0.5384372797817355\n",
      "2019-11-20 00:36:57,515 - sequence_tagger_bert - INFO - Validation metrics: (0.4773006134969324,)\n",
      "2019-11-20 00:36:57,534 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 20/20 [18:13<00:00, 54.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:37:00,991 - sequence_tagger_bert - INFO - Entity-level f1: 0.34105653382761814\n",
      "2019-11-20 00:37:00,992 - sequence_tagger_bert - INFO - Token-level f1: 0.3320134023758757\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 10\n",
    "PRED_BATCH_SIZE = 10\n",
    "MAX_N_EPOCHS = 20\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "\n",
    "from bert_sequence_tagger.bert_utils import create_loader_from_flair_corpus, get_parameters_without_decay\n",
    "from bert_sequence_tagger.model_trainer_bert import ModelTrainerBert\n",
    "\n",
    "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
    "\n",
    "from bert_sequence_tagger.metrics import f1_entity_level, f1_token_level\n",
    "\n",
    "test_dataloader = create_loader_from_flair_corpus(corpus.test,\n",
    "                                                  SequentialSampler,\n",
    "                                                  batch_size=PRED_BATCH_SIZE)\n",
    "\n",
    "_, __, test_metrics = seq_tagger.predict(test_dataloader, evaluate=True, \n",
    "                                         metrics=[f1_entity_level, f1_token_level])\n",
    "logger.info(f'Entity-level f1: {test_metrics[1]}')\n",
    "logger.info(f'Token-level f1: {test_metrics[2]}')\n",
    "\n",
    "train_dataloader = create_loader_from_flair_corpus(corpus.train, \n",
    "                                                   RandomSampler, \n",
    "                                                   batch_size=BATCH_SIZE)\n",
    "val_dataloader = create_loader_from_flair_corpus(corpus.dev,\n",
    "                                                 SequentialSampler,\n",
    "                                                 batch_size=PRED_BATCH_SIZE)\n",
    "\n",
    "optimizer = AdamW(get_parameters_without_decay(model), lr=LEARNING_RATE, betas=(0.9, 0.999), \n",
    "                  eps =1e-6, weight_decay=0.01, correct_bias=True)\n",
    "lr_scheduler = WarmupLinearSchedule(optimizer, warmup_steps=0.1, \n",
    "                                    t_total=(len(corpus.train) / BATCH_SIZE)*MAX_N_EPOCHS)\n",
    "trainer = ModelTrainerBert(model=seq_tagger, \n",
    "                           optimizer=optimizer, \n",
    "                           lr_scheduler=lr_scheduler,\n",
    "                           train_dataloader=train_dataloader, \n",
    "                           val_dataloader=val_dataloader,\n",
    "                           update_scheduler='es',\n",
    "                           keep_best_model=True,\n",
    "                           restore_bm_on_lr_change=False,\n",
    "                           max_grad_norm=1.,\n",
    "                           validation_metrics=[f1_entity_level],\n",
    "                           decision_metric=lambda metrics: -metrics[1])\n",
    "\n",
    "trainer.train(epochs=MAX_N_EPOCHS)\n",
    "\n",
    "\n",
    "test_dataloader = create_loader_from_flair_corpus(corpus.test,\n",
    "                                                  SequentialSampler,\n",
    "                                                  batch_size=PRED_BATCH_SIZE)\n",
    "\n",
    "_, __, test_metrics = seq_tagger.predict(test_dataloader, evaluate=True, \n",
    "                                         metrics=[f1_entity_level, f1_token_level])\n",
    "logger.info(f'Entity-level f1: {test_metrics[1]}')\n",
    "logger.info(f'Token-level f1: {test_metrics[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('labels_v3_predful_manual.txt', 'w') as f:\n",
    "    for _string in _:\n",
    "        #f.seek(0)\n",
    "        f.write(', '.join(_string) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_flair_corpus(corpus, name='tag', filter_tokens={'-DOCSTART-'}):\n",
    "    result = []\n",
    "    for sent in corpus[:10]:\n",
    "        print (\"sent\", sent)\n",
    "        print (\"sent[0].text\", sent[0].text)\n",
    "        if sent[0].text in filter_tokens:\n",
    "            continue\n",
    "        else:\n",
    "            result.append(([token.text for token in sent.tokens],\n",
    "                           [token.tags[name].value for token in sent.tokens]))\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = create_loader_from_flair_corpus(corpus.test,\n",
    "                                                  SequentialSampler,\n",
    "                                                  batch_size=PRED_BATCH_SIZE)\n",
    "\n",
    "_, __, test_metrics = seq_tagger.predict(test_dataloader, evaluate=True, \n",
    "                                         metrics=[f1_entity_level, f1_token_level])\n",
    "logger.info(f'Entity-level f1: {test_metrics[1]}')\n",
    "logger.info(f'Token-level f1: {test_metrics[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.9143007822800387, 0.9306361914074436)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
