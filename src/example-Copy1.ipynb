{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 19 21:43:48 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-DGXS...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   52C    P0   266W / 300W |  19960MiB / 32478MiB |    100%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-DGXS...  On   | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   50C    P0   233W / 300W |  25962MiB / 32478MiB |     81%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-DGXS...  On   | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   46C    P0    67W / 300W |  13516MiB / 32478MiB |     41%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-DGXS...  On   | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   46C    P0    67W / 300W |  23291MiB / 32478MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "logger = logging.getLogger('sequence_tagger_bert')\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "logger.handlers = []\n",
    "\n",
    "fhandler = logging.handlers.TimedRotatingFileHandler(filename='logs.txt', when='midnight')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla V100-DGXS-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda')\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "for i in range(n_gpu):\n",
    "    print(torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = '../workdir/cache'\n",
    "BATCH_SIZE = 16\n",
    "#BATCH_SIZE = 8\n",
    "PRED_BATCH_SIZE = 1000\n",
    "MAX_LEN = 128\n",
    "MAX_N_EPOCHS = 10\n",
    "#MAX_N_EPOCHS = 100\n",
    "#MAX_N_EPOCHS = 50\n",
    "#MAX_N_EPOCHS = 10\n",
    "REDUCE_ON_PLATEAU = False\n",
    "WEIGHT_DECAY = 0.01\n",
    "LEARNING_RATE = 3e-6\n",
    "#LEARNING_RATE = 1e-5\n",
    "#LEARNING_RATE = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/bert_sequence_tagger/src\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:11:51,072 Reading data from /workspace/bert_sequence_tagger/src/data/NER/Varvara_v3\n",
      "2019-11-20 00:11:51,073 Train: /workspace/bert_sequence_tagger/src/data/NER/Varvara_v3/train_pred_full.tsv\n",
      "2019-11-20 00:11:51,074 Dev: /workspace/bert_sequence_tagger/src/data/NER/Varvara_v3/dev_pred_full.tsv\n",
      "2019-11-20 00:11:51,075 Test: /workspace/bert_sequence_tagger/src/data/NER/Varvara_v3/test_manual_predfull_seq_labelling.tsv\n",
      "{\n",
      "    \"TRAIN\": {\n",
      "        \"dataset\": \"TRAIN\",\n",
      "        \"total_number_of_documents\": 3077,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 89350,\n",
      "            \"min\": 6,\n",
      "            \"max\": 106,\n",
      "            \"avg\": 29.038024049398764\n",
      "        }\n",
      "    },\n",
      "    \"TEST\": {\n",
      "        \"dataset\": \"TEST\",\n",
      "        \"total_number_of_documents\": 488,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 13736,\n",
      "            \"min\": 6,\n",
      "            \"max\": 97,\n",
      "            \"avg\": 28.147540983606557\n",
      "        }\n",
      "    },\n",
      "    \"DEV\": {\n",
      "        \"dataset\": \"DEV\",\n",
      "        \"total_number_of_documents\": 402,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 12360,\n",
      "            \"min\": 8,\n",
      "            \"max\": 102,\n",
      "            \"avg\": 30.746268656716417\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "\n",
    "data_folder = '/workspace/bert_sequence_tagger/src/data/NER/Varvara_v3'\n",
    "corpus = ColumnCorpus(data_folder, \n",
    "                      {0 : 'text', 1 : 'tag'},\n",
    "                      train_file='train_pred_full.tsv',\n",
    "                      test_file='test_manual_predfull_seq_labelling.tsv',\n",
    "                      dev_file='dev_pred_full.tsv')\n",
    "\n",
    "print(corpus.obtain_statistics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = corpus.make_tag_dictionary(tag_type = 'tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'<unk>',\n",
       " b'O',\n",
       " b'B-OBJ',\n",
       " b'B-PREDFULL',\n",
       " b'I-PREDFULL',\n",
       " b'I-OBJ',\n",
       " b'<START>',\n",
       " b'<STOP>']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.idx2item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_sequence_tagger import SequenceTaggerBert, BertForTokenClassificationCustom\n",
    "from pytorch_transformers import BertTokenizer, BertForTokenClassification\n",
    "import torch.nn as nn\n",
    "\n",
    "from bert_sequence_tagger.bert_utils import make_bert_tag_dict_from_flair_corpus\n",
    "\n",
    "\n",
    "bpe_tokenizer = BertTokenizer.from_pretrained('bert-base-cased', cache_dir=None, do_lower_case=False)\n",
    "\n",
    "idx2tag, tag2idx = make_bert_tag_dict_from_flair_corpus(corpus)\n",
    "\n",
    "model = nn.DataParallel(BertForTokenClassificationCustom.from_pretrained('bert-base-cased', cache_dir=None, num_labels=len(tag2idx))).cuda()\n",
    "#model = BertForTokenClassification.from_pretrained('bert-base-cased', cache_dir=CACHE_DIR, num_labels=len(tag2idx)).cuda()\n",
    "\n",
    "seq_tagger = SequenceTaggerBert(bert_model=model, bpe_tokenizer=bpe_tokenizer, idx2tag=idx2tag, tag2idx=tag2idx, max_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pytorch_transformers.tokenization_bert.BertTokenizer at 0x7f6250ad0390>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:18:43,851 - sequence_tagger_bert - INFO - Entity-level f1: 0.032533889468196034\n",
      "2019-11-20 00:18:43,852 - sequence_tagger_bert - INFO - Token-level f1: 0.06207475067573866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:19:36,479 - sequence_tagger_bert - INFO - Train loss: 0.3619607299953312\n",
      "2019-11-20 00:19:39,953 - sequence_tagger_bert - INFO - Validation loss: 0.3088280222401386\n",
      "2019-11-20 00:19:39,954 - sequence_tagger_bert - INFO - Validation metrics: (0.3985239852398525,)\n",
      "2019-11-20 00:19:39,973 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   5%|▌         | 1/20 [00:56<17:44, 56.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:20:32,119 - sequence_tagger_bert - INFO - Train loss: 0.08740380431174652\n",
      "2019-11-20 00:20:34,944 - sequence_tagger_bert - INFO - Validation loss: 0.39300361003090695\n",
      "2019-11-20 00:20:34,945 - sequence_tagger_bert - INFO - Validation metrics: (0.4347826086956522,)\n",
      "2019-11-20 00:20:34,964 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|█         | 2/20 [01:51<16:43, 55.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:21:26,956 - sequence_tagger_bert - INFO - Train loss: 0.06447363679309363\n",
      "2019-11-20 00:21:29,632 - sequence_tagger_bert - INFO - Validation loss: 0.42421448148968743\n",
      "2019-11-20 00:21:29,633 - sequence_tagger_bert - INFO - Validation metrics: (0.4388078630310716,)\n",
      "2019-11-20 00:21:29,650 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  15%|█▌        | 3/20 [02:45<15:42, 55.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:22:22,670 - sequence_tagger_bert - INFO - Train loss: 0.05221280366992699\n",
      "2019-11-20 00:22:25,633 - sequence_tagger_bert - INFO - Validation loss: 0.43800021946521067\n",
      "2019-11-20 00:22:25,635 - sequence_tagger_bert - INFO - Validation metrics: (0.44360428481411474,)\n",
      "2019-11-20 00:22:25,659 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|██        | 4/20 [03:41<14:49, 55.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:23:17,594 - sequence_tagger_bert - INFO - Train loss: 0.04596274611902992\n",
      "2019-11-20 00:23:20,634 - sequence_tagger_bert - INFO - Validation loss: 0.4470692996344552\n",
      "2019-11-20 00:23:20,635 - sequence_tagger_bert - INFO - Validation metrics: (0.45316455696202534,)\n",
      "2019-11-20 00:23:20,654 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  25%|██▌       | 5/20 [04:36<13:51, 55.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:24:11,892 - sequence_tagger_bert - INFO - Train loss: 0.04020843446532917\n",
      "2019-11-20 00:24:14,878 - sequence_tagger_bert - INFO - Validation loss: 0.48331384674259803\n",
      "2019-11-20 00:24:14,879 - sequence_tagger_bert - INFO - Validation metrics: (0.45218492716909436,)\n",
      "2019-11-20 00:24:14,880 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|███       | 6/20 [05:30<12:50, 55.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:25:06,428 - sequence_tagger_bert - INFO - Train loss: 0.0358084871920503\n",
      "2019-11-20 00:25:09,618 - sequence_tagger_bert - INFO - Validation loss: 0.48216514689166373\n",
      "2019-11-20 00:25:09,619 - sequence_tagger_bert - INFO - Validation metrics: (0.4474829086389062,)\n",
      "2019-11-20 00:25:09,621 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  35%|███▌      | 7/20 [06:25<11:54, 54.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:26:01,430 - sequence_tagger_bert - INFO - Train loss: 0.0332889553392306\n",
      "2019-11-20 00:26:04,259 - sequence_tagger_bert - INFO - Validation loss: 0.5220233976909119\n",
      "2019-11-20 00:26:04,261 - sequence_tagger_bert - INFO - Validation metrics: (0.46222222222222226,)\n",
      "2019-11-20 00:26:04,281 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|████      | 8/20 [07:20<10:58, 54.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:26:57,505 - sequence_tagger_bert - INFO - Train loss: 0.03205127487421737\n",
      "2019-11-20 00:27:00,557 - sequence_tagger_bert - INFO - Validation loss: 0.5088469949096623\n",
      "2019-11-20 00:27:00,558 - sequence_tagger_bert - INFO - Validation metrics: (0.46134347275031684,)\n",
      "2019-11-20 00:27:00,559 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  45%|████▌     | 9/20 [08:16<10:08, 55.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:27:52,452 - sequence_tagger_bert - INFO - Train loss: 0.029160591297444294\n",
      "2019-11-20 00:27:55,382 - sequence_tagger_bert - INFO - Validation loss: 0.48546486625039\n",
      "2019-11-20 00:27:55,383 - sequence_tagger_bert - INFO - Validation metrics: (0.46892307692307694,)\n",
      "2019-11-20 00:27:55,403 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████     | 10/20 [09:11<09:11, 55.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:28:47,639 - sequence_tagger_bert - INFO - Train loss: 0.026940659996327738\n",
      "2019-11-20 00:28:50,424 - sequence_tagger_bert - INFO - Validation loss: 0.5129956489360732\n",
      "2019-11-20 00:28:50,425 - sequence_tagger_bert - INFO - Validation metrics: (0.47160493827160493,)\n",
      "2019-11-20 00:28:50,444 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  55%|█████▌    | 11/20 [10:06<08:16, 55.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:29:41,321 - sequence_tagger_bert - INFO - Train loss: 0.025348623533424763\n",
      "2019-11-20 00:29:44,479 - sequence_tagger_bert - INFO - Validation loss: 0.5219396179905389\n",
      "2019-11-20 00:29:44,480 - sequence_tagger_bert - INFO - Validation metrics: (0.471100062150404,)\n",
      "2019-11-20 00:29:44,482 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|██████    | 12/20 [11:00<07:18, 54.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:30:36,152 - sequence_tagger_bert - INFO - Train loss: 0.02420229448667764\n",
      "2019-11-20 00:30:38,889 - sequence_tagger_bert - INFO - Validation loss: 0.5160302322176171\n",
      "2019-11-20 00:30:38,890 - sequence_tagger_bert - INFO - Validation metrics: (0.47095179233621753,)\n",
      "2019-11-20 00:30:38,891 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  65%|██████▌   | 13/20 [11:54<06:22, 54.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:31:31,216 - sequence_tagger_bert - INFO - Train loss: 0.0222365621215809\n",
      "2019-11-20 00:31:33,956 - sequence_tagger_bert - INFO - Validation loss: 0.5339932873924603\n",
      "2019-11-20 00:31:33,957 - sequence_tagger_bert - INFO - Validation metrics: (0.47051520794537555,)\n",
      "2019-11-20 00:31:33,958 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|███████   | 14/20 [12:50<05:28, 54.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:32:25,028 - sequence_tagger_bert - INFO - Train loss: 0.02142074331414414\n",
      "2019-11-20 00:32:28,010 - sequence_tagger_bert - INFO - Validation loss: 0.5332436034365035\n",
      "2019-11-20 00:32:28,011 - sequence_tagger_bert - INFO - Validation metrics: (0.4747225647348952,)\n",
      "2019-11-20 00:32:28,029 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  75%|███████▌  | 15/20 [13:44<04:32, 54.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:33:19,202 - sequence_tagger_bert - INFO - Train loss: 0.0203145507642963\n",
      "2019-11-20 00:33:22,066 - sequence_tagger_bert - INFO - Validation loss: 0.5433915181187685\n",
      "2019-11-20 00:33:22,067 - sequence_tagger_bert - INFO - Validation metrics: (0.47211895910780666,)\n",
      "2019-11-20 00:33:22,067 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|████████  | 16/20 [14:38<03:37, 54.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:34:12,922 - sequence_tagger_bert - INFO - Train loss: 0.01986186942214134\n",
      "2019-11-20 00:34:15,760 - sequence_tagger_bert - INFO - Validation loss: 0.5410953026049111\n",
      "2019-11-20 00:34:15,761 - sequence_tagger_bert - INFO - Validation metrics: (0.4740740740740741,)\n",
      "2019-11-20 00:34:15,761 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  85%|████████▌ | 17/20 [15:31<02:42, 54.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:35:06,510 - sequence_tagger_bert - INFO - Train loss: 0.01940659679341668\n",
      "2019-11-20 00:35:09,299 - sequence_tagger_bert - INFO - Validation loss: 0.5366039614920027\n",
      "2019-11-20 00:35:09,300 - sequence_tagger_bert - INFO - Validation metrics: (0.4763076923076923,)\n",
      "2019-11-20 00:35:09,324 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  90%|█████████ | 18/20 [16:25<01:48, 54.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:36:01,028 - sequence_tagger_bert - INFO - Train loss: 0.018303427412839874\n",
      "2019-11-20 00:36:03,860 - sequence_tagger_bert - INFO - Validation loss: 0.5431420835098479\n",
      "2019-11-20 00:36:03,991 - sequence_tagger_bert - INFO - Validation metrics: (0.47466007416563666,)\n",
      "2019-11-20 00:36:03,992 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  95%|█████████▌| 19/20 [17:20<00:54, 54.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:36:54,500 - sequence_tagger_bert - INFO - Train loss: 0.018760964634991554\n",
      "2019-11-20 00:36:57,514 - sequence_tagger_bert - INFO - Validation loss: 0.5384372797817355\n",
      "2019-11-20 00:36:57,515 - sequence_tagger_bert - INFO - Validation metrics: (0.4773006134969324,)\n",
      "2019-11-20 00:36:57,534 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 20/20 [18:13<00:00, 54.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-20 00:37:00,991 - sequence_tagger_bert - INFO - Entity-level f1: 0.34105653382761814\n",
      "2019-11-20 00:37:00,992 - sequence_tagger_bert - INFO - Token-level f1: 0.3320134023758757\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 10\n",
    "PRED_BATCH_SIZE = 10\n",
    "MAX_N_EPOCHS = 20\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "\n",
    "from bert_sequence_tagger.bert_utils import create_loader_from_flair_corpus, get_parameters_without_decay\n",
    "from bert_sequence_tagger.model_trainer_bert import ModelTrainerBert\n",
    "\n",
    "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
    "\n",
    "from bert_sequence_tagger.metrics import f1_entity_level, f1_token_level\n",
    "\n",
    "test_dataloader = create_loader_from_flair_corpus(corpus.test,\n",
    "                                                  SequentialSampler,\n",
    "                                                  batch_size=PRED_BATCH_SIZE)\n",
    "\n",
    "_, __, test_metrics = seq_tagger.predict(test_dataloader, evaluate=True, \n",
    "                                         metrics=[f1_entity_level, f1_token_level])\n",
    "logger.info(f'Entity-level f1: {test_metrics[1]}')\n",
    "logger.info(f'Token-level f1: {test_metrics[2]}')\n",
    "\n",
    "train_dataloader = create_loader_from_flair_corpus(corpus.train, \n",
    "                                                   RandomSampler, \n",
    "                                                   batch_size=BATCH_SIZE)\n",
    "val_dataloader = create_loader_from_flair_corpus(corpus.dev,\n",
    "                                                 SequentialSampler,\n",
    "                                                 batch_size=PRED_BATCH_SIZE)\n",
    "\n",
    "optimizer = AdamW(get_parameters_without_decay(model), lr=LEARNING_RATE, betas=(0.9, 0.999), \n",
    "                  eps =1e-6, weight_decay=0.01, correct_bias=True)\n",
    "lr_scheduler = WarmupLinearSchedule(optimizer, warmup_steps=0.1, \n",
    "                                    t_total=(len(corpus.train) / BATCH_SIZE)*MAX_N_EPOCHS)\n",
    "trainer = ModelTrainerBert(model=seq_tagger, \n",
    "                           optimizer=optimizer, \n",
    "                           lr_scheduler=lr_scheduler,\n",
    "                           train_dataloader=train_dataloader, \n",
    "                           val_dataloader=val_dataloader,\n",
    "                           update_scheduler='es',\n",
    "                           keep_best_model=True,\n",
    "                           restore_bm_on_lr_change=False,\n",
    "                           max_grad_norm=1.,\n",
    "                           validation_metrics=[f1_entity_level],\n",
    "                           decision_metric=lambda metrics: -metrics[1])\n",
    "\n",
    "trainer.train(epochs=MAX_N_EPOCHS)\n",
    "\n",
    "\n",
    "test_dataloader = create_loader_from_flair_corpus(corpus.test,\n",
    "                                                  SequentialSampler,\n",
    "                                                  batch_size=PRED_BATCH_SIZE)\n",
    "\n",
    "_, __, test_metrics = seq_tagger.predict(test_dataloader, evaluate=True, \n",
    "                                         metrics=[f1_entity_level, f1_token_level])\n",
    "logger.info(f'Entity-level f1: {test_metrics[1]}')\n",
    "logger.info(f'Token-level f1: {test_metrics[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('labels_v3_predful_manual.txt', 'w') as f:\n",
    "    for _string in _:\n",
    "        #f.seek(0)\n",
    "        f.write(', '.join(_string) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_flair_corpus(corpus, name='tag', filter_tokens={'-DOCSTART-'}):\n",
    "    result = []\n",
    "    for sent in corpus[:10]:\n",
    "        print (\"sent\", sent)\n",
    "        print (\"sent[0].text\", sent[0].text)\n",
    "        if sent[0].text in filter_tokens:\n",
    "            continue\n",
    "        else:\n",
    "            result.append(([token.text for token in sent.tokens],\n",
    "                           [token.tags[name].value for token in sent.tokens]))\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = create_loader_from_flair_corpus(corpus.test,\n",
    "                                                  SequentialSampler,\n",
    "                                                  batch_size=PRED_BATCH_SIZE)\n",
    "\n",
    "_, __, test_metrics = seq_tagger.predict(test_dataloader, evaluate=True, \n",
    "                                         metrics=[f1_entity_level, f1_token_level])\n",
    "logger.info(f'Entity-level f1: {test_metrics[1]}')\n",
    "logger.info(f'Token-level f1: {test_metrics[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.9143007822800387, 0.9306361914074436)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluatorBase():\n",
    "    \"\"\"EvaluatorBase is abstract base class for all evaluators\"\"\"\n",
    "    def get_evaluation_score_train_dev_test(self, tagger, datasets_bank, batch_size=-1):\n",
    "        if batch_size == -1:\n",
    "            batch_size = tagger.batch_size\n",
    "        score_train, _ = self.predict_evaluation_score(tagger=tagger,\n",
    "                                                       word_sequences=datasets_bank.word_sequences_train,\n",
    "                                                       targets_tag_sequences=datasets_bank.tag_sequences_train,\n",
    "                                                       batch_size=batch_size)\n",
    "        score_dev, _ = self.predict_evaluation_score(tagger=tagger,\n",
    "                                                     word_sequences=datasets_bank.word_sequences_dev,\n",
    "                                                     targets_tag_sequences=datasets_bank.tag_sequences_dev,\n",
    "                                                     batch_size=batch_size)\n",
    "        score_test, msg_test = self.predict_evaluation_score(tagger=tagger,\n",
    "                                                             word_sequences=datasets_bank.word_sequences_test,\n",
    "                                                             targets_tag_sequences=datasets_bank.tag_sequences_test,\n",
    "                                                             batch_size=batch_size)\n",
    "        return score_train, score_dev, score_test, msg_test\n",
    "\n",
    "    def predict_evaluation_score(self, tagger, word_sequences, targets_tag_sequences, batch_size):\n",
    "        outputs_tag_sequences = tagger.predict_tags_from_words(word_sequences, batch_size)\n",
    "        return self.get_evaluation_score(targets_tag_sequences, outputs_tag_sequences, word_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluatorF1MacroTokenLevel(EvaluatorBase):\n",
    "    def __init__(self):\n",
    "        self.tag_list = None\n",
    "        self.tag2idx = dict()\n",
    "\n",
    "    def __init_tag_list(self, targets_tag_sequences):\n",
    "        if self.tag_list is not None:\n",
    "            return\n",
    "        self.tag_list = list()\n",
    "        for tag_seq in targets_tag_sequences:\n",
    "            for t in tag_seq:\n",
    "                if t not in self.tag_list:\n",
    "                    self.tag_list.append(t)\n",
    "                    self.tag2idx[t] = len(self.tag_list)\n",
    "        self.tag_list.sort()\n",
    "\n",
    "    def tag_seq_2_idx_list(self, tag_seq):\n",
    "        return [self.tag2idx[t] for t in tag_seq]\n",
    "\n",
    "    def __get_zeros_tag_dict(self):\n",
    "        return {tag: 0 for tag in self.tag_list}\n",
    "\n",
    "    def __add_dict(self, dict1, dict2):\n",
    "        for tag in self.tag_list:\n",
    "            dict1[tag] += dict2[tag]\n",
    "        return dict1\n",
    "\n",
    "    def __div_dict(self, dict, d):\n",
    "        for tag in self.tag_list:\n",
    "            dict[tag] /= d\n",
    "        return dict\n",
    "\n",
    "    def __get_M_F1_msg(self, F1, precision, recall):\n",
    "        msg = '\\nF1 scores\\n'\n",
    "        msg += '-' * 24 + '\\n'\n",
    "        sum_M_F1 = 0\n",
    "        sum_precision = 0\n",
    "        sum_recall = 0\n",
    "        for tag in self.tag_list:\n",
    "            sum_M_F1 += F1[tag]\n",
    "            sum_precision += precision[tag]\n",
    "            sum_recall += recall[tag]\n",
    "            msg += '%15s = f1 = %1.2f, precision = %1.2f, recall = %1.2f\\n' % (tag, F1[tag], precision[tag], recall[tag])\n",
    "        M_F1 = sum_M_F1 / len(F1)\n",
    "        M_PR = sum_precision / len(F1)\n",
    "        M_RE = sum_recall / len(F1)\n",
    "        msg += '-'*24 + '\\n'\n",
    "        msg += 'Macro-F1 = %1.3f' % M_F1\n",
    "        msg += 'Macro-Prescion = %1.3f' % M_PR\n",
    "        msg += 'Macro-Recall = %1.3f' % M_RE\n",
    "        return M_F1, msg\n",
    "\n",
    "    def __add_to_dict(self, dict_in, tag, val):\n",
    "        if tag in dict_in:\n",
    "            dict_in[tag] += val\n",
    "        else:\n",
    "            dict_in[tag] = val\n",
    "        return dict_in\n",
    "\n",
    "    \"\"\"EvaluatorF1MacroTagComponents is macro-F1 scores evaluator for each class of BOI-like tags.\"\"\"\n",
    "    def get_evaluation_score(self, targets_tag_sequences, outputs_tag_sequences, word_sequences=None):\n",
    "        # Create list of tags\n",
    "        self.__init_tag_list(targets_tag_sequences)\n",
    "        i = 0\n",
    "        for elem in zip(targets_tag_sequences, outputs_tag_sequences):\n",
    "            if (i < 4):\n",
    "                i = i +1\n",
    "                print (elem[0])\n",
    "                print (elem[1])\n",
    "        # Init values\n",
    "        TP = self.__get_zeros_tag_dict()\n",
    "        FP = self.__get_zeros_tag_dict()\n",
    "        FN = self.__get_zeros_tag_dict()\n",
    "        F1 = self.__get_zeros_tag_dict()\n",
    "        precision = self.__get_zeros_tag_dict()\n",
    "        recall = self.__get_zeros_tag_dict()\n",
    "        for targets_seq, outputs_tag_seq in zip(targets_tag_sequences, outputs_tag_sequences):\n",
    "            for t, o in zip(targets_seq, outputs_tag_seq):\n",
    "                if t == o:\n",
    "                    TP = self.__add_to_dict(TP, t, 1)\n",
    "                else:\n",
    "                    FN = self.__add_to_dict(FN, t, 1)\n",
    "                    FP = self.__add_to_dict(FP, o, 1)\n",
    "        # Calculate F1 for each tag\n",
    "        for tag in self.tag_list:\n",
    "            F1[tag] = (2 * TP[tag] / max(2 * TP[tag] + FP[tag] + FN[tag], 1)) * 100\n",
    "            precision[tag] = (TP[tag] / max(TP[tag] + FP[tag], 1))*100\n",
    "            recall[tag] = (TP[tag] / max(TP[tag] + FN[tag], 1))*100\n",
    "        # Calculate Macro-F1 score and prepare the message\n",
    "        M_F1, msg = self.__get_M_F1_msg(F1,precision, recall)\n",
    "        print(msg)\n",
    "        #self.validate_M_F1_scikitlearn( targets_tag_sequences, outputs_tag_sequences)\n",
    "        return M_F1, msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
