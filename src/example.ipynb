{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov  3 23:11:09 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-DGXS...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   42C    P0    65W / 300W |  27725MiB / 32478MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-DGXS...  On   | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    51W / 300W |   1920MiB / 32478MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-DGXS...  On   | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   42C    P0    77W / 300W |  14817MiB / 32478MiB |     79%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-DGXS...  On   | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   42C    P0    52W / 300W |   3537MiB / 32478MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "logger = logging.getLogger('sequence_tagger_bert')\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "logger.handlers = []\n",
    "\n",
    "fhandler = logging.handlers.TimedRotatingFileHandler(filename='logs.txt', when='midnight')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla V100-DGXS-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda')\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "for i in range(n_gpu):\n",
    "    print(torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = '../workdir/cache'\n",
    "BATCH_SIZE = 16\n",
    "#BATCH_SIZE = 8\n",
    "PRED_BATCH_SIZE = 1000\n",
    "MAX_LEN = 128\n",
    "MAX_N_EPOCHS = 20\n",
    "#MAX_N_EPOCHS = 100\n",
    "#MAX_N_EPOCHS = 50\n",
    "#MAX_N_EPOCHS = 10\n",
    "REDUCE_ON_PLATEAU = False\n",
    "WEIGHT_DECAY = 0.01\n",
    "LEARNING_RATE = 3e-7\n",
    "#LEARNING_RATE = 1e-5\n",
    "#LEARNING_RATE = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_aspect.tsv\t   test_aspect.tsv     train_aspect.tsv\r\n",
      "dev_pred_full.tsv  test_pred_full.tsv  train_pred_full.tsv\r\n"
     ]
    }
   ],
   "source": [
    "!ls /workspace/bert_sequence_tagger/src/data/NER/Varvara_v1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-03 23:11:51,229 Reading data from /workspace/bert_sequence_tagger/src/data/NER/Varvara_v3\n",
      "2019-11-03 23:11:51,230 Train: /workspace/bert_sequence_tagger/src/data/NER/Varvara_v3/train_pred_full.tsv\n",
      "2019-11-03 23:11:51,230 Dev: /workspace/bert_sequence_tagger/src/data/NER/Varvara_v3/dev_pred_full.tsv\n",
      "2019-11-03 23:11:51,231 Test: /workspace/bert_sequence_tagger/src/data/NER/Varvara_v3/test_pred_full.tsv\n",
      "{\n",
      "    \"TRAIN\": {\n",
      "        \"dataset\": \"TRAIN\",\n",
      "        \"total_number_of_documents\": 3077,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 89350,\n",
      "            \"min\": 6,\n",
      "            \"max\": 106,\n",
      "            \"avg\": 29.038024049398764\n",
      "        }\n",
      "    },\n",
      "    \"TEST\": {\n",
      "        \"dataset\": \"TEST\",\n",
      "        \"total_number_of_documents\": 488,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 13736,\n",
      "            \"min\": 6,\n",
      "            \"max\": 97,\n",
      "            \"avg\": 28.147540983606557\n",
      "        }\n",
      "    },\n",
      "    \"DEV\": {\n",
      "        \"dataset\": \"DEV\",\n",
      "        \"total_number_of_documents\": 402,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 12360,\n",
      "            \"min\": 8,\n",
      "            \"max\": 102,\n",
      "            \"avg\": 30.746268656716417\n",
      "        }\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"TRAIN\": {\n",
      "        \"dataset\": \"TRAIN\",\n",
      "        \"total_number_of_documents\": 3077,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 89350,\n",
      "            \"min\": 6,\n",
      "            \"max\": 106,\n",
      "            \"avg\": 29.038024049398764\n",
      "        }\n",
      "    },\n",
      "    \"TEST\": {\n",
      "        \"dataset\": \"TEST\",\n",
      "        \"total_number_of_documents\": 488,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 13736,\n",
      "            \"min\": 6,\n",
      "            \"max\": 97,\n",
      "            \"avg\": 28.147540983606557\n",
      "        }\n",
      "    },\n",
      "    \"DEV\": {\n",
      "        \"dataset\": \"DEV\",\n",
      "        \"total_number_of_documents\": 402,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 12360,\n",
      "            \"min\": 8,\n",
      "            \"max\": 102,\n",
      "            \"avg\": 30.746268656716417\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "\n",
    "data_folder = '/workspace/bert_sequence_tagger/src/data/NER/Varvara_v3'\n",
    "corpus = ColumnCorpus(data_folder, \n",
    "                      {0 : 'text', 1 : 'tag'},\n",
    "                      train_file='train_pred_full.tsv',\n",
    "                      test_file='test_pred_full.tsv',\n",
    "                      dev_file='dev_pred_full.tsv')\n",
    "\n",
    "print(corpus.obtain_statistics())\n",
    "\n",
    "print(corpus.obtain_statistics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = corpus.make_tag_dictionary(tag_type = 'tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'<unk>',\n",
       " b'O',\n",
       " b'B-OBJ',\n",
       " b'B-PREDFULL',\n",
       " b'I-PREDFULL',\n",
       " b'I-OBJ',\n",
       " b'<START>',\n",
       " b'<STOP>']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.idx2item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_sequence_tagger import SequenceTaggerBert, BertForTokenClassificationCustom\n",
    "from pytorch_transformers import BertTokenizer, BertForTokenClassification\n",
    "import torch.nn as nn\n",
    "\n",
    "from bert_sequence_tagger.bert_utils import make_bert_tag_dict_from_flair_corpus\n",
    "\n",
    "\n",
    "bpe_tokenizer = BertTokenizer.from_pretrained('bert-base-cased', cache_dir=None, do_lower_case=False)\n",
    "\n",
    "idx2tag, tag2idx = make_bert_tag_dict_from_flair_corpus(corpus)\n",
    "\n",
    "model = nn.DataParallel(BertForTokenClassificationCustom.from_pretrained('bert-base-cased', cache_dir=None, num_labels=len(tag2idx))).cuda()\n",
    "#model = BertForTokenClassification.from_pretrained('bert-base-cased', cache_dir=CACHE_DIR, num_labels=len(tag2idx)).cuda()\n",
    "\n",
    "seq_tagger = SequenceTaggerBert(bert_model=model, bpe_tokenizer=bpe_tokenizer, idx2tag=idx2tag, tag2idx=tag2idx, max_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-04 00:15:05,500 - sequence_tagger_bert - INFO - Entity-level f1: 0.5359922178988327\n",
      "2019-11-04 00:15:05,502 - sequence_tagger_bert - INFO - Token-level f1: 0.5660714285714287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-04 00:16:12,843 - sequence_tagger_bert - INFO - Train loss: 0.02296537380097071\n",
      "2019-11-04 00:16:15,939 - sequence_tagger_bert - INFO - Validation loss: 0.5064210319641705\n",
      "2019-11-04 00:16:15,940 - sequence_tagger_bert - INFO - Validation metrics: (0.4930847865303668,)\n",
      "2019-11-04 00:16:15,965 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:   5%|▌         | 1/20 [01:10<22:17, 70.38s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-04 00:17:23,664 - sequence_tagger_bert - INFO - Train loss: 0.02211489531630348\n",
      "2019-11-04 00:17:26,546 - sequence_tagger_bert - INFO - Validation loss: 0.5068650401946975\n",
      "2019-11-04 00:17:26,547 - sequence_tagger_bert - INFO - Validation metrics: (0.5014961101137044,)\n",
      "2019-11-04 00:17:26,565 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:  10%|█         | 2/20 [02:20<21:08, 70.45s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-04 00:18:33,674 - sequence_tagger_bert - INFO - Train loss: 0.022489066854083015\n",
      "2019-11-04 00:18:36,804 - sequence_tagger_bert - INFO - Validation loss: 0.5107156120058967\n",
      "2019-11-04 00:18:36,805 - sequence_tagger_bert - INFO - Validation metrics: (0.49489489489489497,)\n",
      "2019-11-04 00:18:36,806 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:  15%|█▌        | 3/20 [03:31<19:56, 70.38s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-04 00:19:43,636 - sequence_tagger_bert - INFO - Train loss: 0.021868705845894534\n",
      "2019-11-04 00:19:46,608 - sequence_tagger_bert - INFO - Validation loss: 0.5176699931744668\n",
      "2019-11-04 00:19:46,609 - sequence_tagger_bert - INFO - Validation metrics: (0.49063444108761334,)\n",
      "2019-11-04 00:19:46,609 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:  20%|██        | 4/20 [04:41<18:43, 70.21s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-04 00:20:53,377 - sequence_tagger_bert - INFO - Train loss: 0.021474948615159282\n",
      "2019-11-04 00:20:56,415 - sequence_tagger_bert - INFO - Validation loss: 0.5129166986145897\n",
      "2019-11-04 00:20:56,416 - sequence_tagger_bert - INFO - Validation metrics: (0.4993997599039616,)\n",
      "2019-11-04 00:20:56,417 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:  25%|██▌       | 5/20 [05:50<17:31, 70.09s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-04 00:22:03,707 - sequence_tagger_bert - INFO - Train loss: 0.02082470166247471\n",
      "2019-11-04 00:22:06,635 - sequence_tagger_bert - INFO - Validation loss: 0.5123378931624224\n",
      "2019-11-04 00:22:06,637 - sequence_tagger_bert - INFO - Validation metrics: (0.4969987995198079,)\n",
      "2019-11-04 00:22:06,637 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:  30%|███       | 6/20 [07:01<16:21, 70.13s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-04 00:23:13,760 - sequence_tagger_bert - INFO - Train loss: 0.020918038098047813\n",
      "2019-11-04 00:23:16,734 - sequence_tagger_bert - INFO - Validation loss: 0.5116124798986698\n",
      "2019-11-04 00:23:16,735 - sequence_tagger_bert - INFO - Validation metrics: (0.4994026284348865,)\n",
      "2019-11-04 00:23:16,736 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:  35%|███▌      | 7/20 [08:11<15:11, 70.12s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-04 00:24:23,733 - sequence_tagger_bert - INFO - Train loss: 0.020607874843642696\n",
      "2019-11-04 00:24:26,704 - sequence_tagger_bert - INFO - Validation loss: 0.5216088233822275\n",
      "2019-11-04 00:24:26,705 - sequence_tagger_bert - INFO - Validation metrics: (0.49849488260084285,)\n",
      "2019-11-04 00:24:26,706 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:  40%|████      | 8/20 [09:21<14:00, 70.07s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-04 00:25:34,201 - sequence_tagger_bert - INFO - Train loss: 0.020006767906136688\n",
      "2019-11-04 00:25:37,417 - sequence_tagger_bert - INFO - Validation loss: 0.5229435328581575\n",
      "2019-11-04 00:25:37,418 - sequence_tagger_bert - INFO - Validation metrics: (0.49969969969969963,)\n",
      "2019-11-04 00:25:37,419 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:  45%|████▌     | 9/20 [10:31<12:52, 70.27s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-04 00:26:45,107 - sequence_tagger_bert - INFO - Train loss: 0.02056950890812902\n",
      "2019-11-04 00:26:48,139 - sequence_tagger_bert - INFO - Validation loss: 0.5140314608948623\n",
      "2019-11-04 00:26:48,140 - sequence_tagger_bert - INFO - Validation metrics: (0.5035714285714286,)\n",
      "2019-11-04 00:26:48,160 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:  50%|█████     | 10/20 [11:42<11:44, 70.41s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-04 00:27:54,893 - sequence_tagger_bert - INFO - Train loss: 0.019477534391342772\n",
      "2019-11-04 00:27:57,966 - sequence_tagger_bert - INFO - Validation loss: 0.5256900111548375\n",
      "2019-11-04 00:27:57,967 - sequence_tagger_bert - INFO - Validation metrics: (0.49939540507859737,)\n",
      "2019-11-04 00:27:57,969 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:  55%|█████▌    | 11/20 [12:52<10:32, 70.23s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-04 00:29:05,567 - sequence_tagger_bert - INFO - Train loss: 0.020042150701674356\n",
      "2019-11-04 00:29:08,487 - sequence_tagger_bert - INFO - Validation loss: 0.5230054469557661\n",
      "2019-11-04 00:29:08,488 - sequence_tagger_bert - INFO - Validation metrics: (0.4984984984984986,)\n",
      "2019-11-04 00:29:08,489 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:  60%|██████    | 12/20 [14:02<09:22, 70.32s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-04 00:30:14,667 - sequence_tagger_bert - INFO - Train loss: 0.020259920838016823\n",
      "2019-11-04 00:30:17,684 - sequence_tagger_bert - INFO - Validation loss: 0.524547247911758\n",
      "2019-11-04 00:30:17,685 - sequence_tagger_bert - INFO - Validation metrics: (0.4969843184559712,)\n",
      "2019-11-04 00:30:17,686 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:  65%|██████▌   | 13/20 [15:12<08:09, 69.98s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-04 00:31:25,581 - sequence_tagger_bert - INFO - Train loss: 0.01906152820296271\n",
      "2019-11-04 00:31:28,617 - sequence_tagger_bert - INFO - Validation loss: 0.5232003611688525\n",
      "2019-11-04 00:31:28,618 - sequence_tagger_bert - INFO - Validation metrics: (0.5002999400119976,)\n",
      "2019-11-04 00:31:28,619 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:  70%|███████   | 14/20 [16:23<07:01, 70.27s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-04 00:32:36,391 - sequence_tagger_bert - INFO - Train loss: 0.019067458004098047\n",
      "2019-11-04 00:32:39,509 - sequence_tagger_bert - INFO - Validation loss: 0.5221355129288873\n",
      "2019-11-04 00:32:39,511 - sequence_tagger_bert - INFO - Validation metrics: (0.5044829647340108,)\n",
      "2019-11-04 00:32:39,530 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:  75%|███████▌  | 15/20 [17:33<05:52, 70.46s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-04 00:33:46,821 - sequence_tagger_bert - INFO - Train loss: 0.019810797411921228\n",
      "2019-11-04 00:33:49,890 - sequence_tagger_bert - INFO - Validation loss: 0.5217976092769787\n",
      "2019-11-04 00:33:49,891 - sequence_tagger_bert - INFO - Validation metrics: (0.502092050209205,)\n",
      "2019-11-04 00:33:49,892 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:  80%|████████  | 16/20 [18:44<04:41, 70.43s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-04 00:34:58,436 - sequence_tagger_bert - INFO - Train loss: 0.01922515010787779\n",
      "2019-11-04 00:35:01,669 - sequence_tagger_bert - INFO - Validation loss: 0.5251691339639719\n",
      "2019-11-04 00:35:01,670 - sequence_tagger_bert - INFO - Validation metrics: (0.4987980769230769,)\n",
      "2019-11-04 00:35:01,671 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:  85%|████████▌ | 17/20 [19:56<03:32, 70.83s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-04 00:36:11,156 - sequence_tagger_bert - INFO - Train loss: 0.019155079037005494\n",
      "2019-11-04 00:36:14,257 - sequence_tagger_bert - INFO - Validation loss: 0.5244880779788307\n",
      "2019-11-04 00:36:14,258 - sequence_tagger_bert - INFO - Validation metrics: (0.5026929982046678,)\n",
      "2019-11-04 00:36:14,259 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:  90%|█████████ | 18/20 [21:08<02:22, 71.36s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-04 00:37:24,729 - sequence_tagger_bert - INFO - Train loss: 0.019374078425409443\n",
      "2019-11-04 00:37:27,703 - sequence_tagger_bert - INFO - Validation loss: 0.5228917853106041\n",
      "2019-11-04 00:37:27,704 - sequence_tagger_bert - INFO - Validation metrics: (0.5023866348448688,)\n",
      "2019-11-04 00:37:27,705 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:  95%|█████████▌| 19/20 [22:22<01:11, 71.99s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-04 00:38:37,752 - sequence_tagger_bert - INFO - Train loss: 0.019024234159634897\n",
      "2019-11-04 00:38:41,119 - sequence_tagger_bert - INFO - Validation loss: 0.5231161052730252\n",
      "2019-11-04 00:38:41,121 - sequence_tagger_bert - INFO - Validation metrics: (0.5014925373134328,)\n",
      "2019-11-04 00:38:41,122 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: 100%|██████████| 20/20 [23:35<00:00, 72.42s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-04 00:38:45,008 - sequence_tagger_bert - INFO - Entity-level f1: 0.5389105058365758\n",
      "2019-11-04 00:38:45,010 - sequence_tagger_bert - INFO - Token-level f1: 0.5688809629959874\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 10\n",
    "PRED_BATCH_SIZE = 10\n",
    "MAX_N_EPOCHS = 20\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "\n",
    "from bert_sequence_tagger.bert_utils import create_loader_from_flair_corpus, get_parameters_without_decay\n",
    "from bert_sequence_tagger.model_trainer_bert import ModelTrainerBert\n",
    "\n",
    "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
    "\n",
    "from bert_sequence_tagger.metrics import f1_entity_level, f1_token_level\n",
    "\n",
    "test_dataloader = create_loader_from_flair_corpus(corpus.test,\n",
    "                                                  SequentialSampler,\n",
    "                                                  batch_size=PRED_BATCH_SIZE)\n",
    "\n",
    "_, __, test_metrics = seq_tagger.predict(test_dataloader, evaluate=True, \n",
    "                                         metrics=[f1_entity_level, f1_token_level])\n",
    "logger.info(f'Entity-level f1: {test_metrics[1]}')\n",
    "logger.info(f'Token-level f1: {test_metrics[2]}')\n",
    "\n",
    "train_dataloader = create_loader_from_flair_corpus(corpus.train, \n",
    "                                                   RandomSampler, \n",
    "                                                   batch_size=BATCH_SIZE)\n",
    "val_dataloader = create_loader_from_flair_corpus(corpus.dev,\n",
    "                                                 SequentialSampler,\n",
    "                                                 batch_size=PRED_BATCH_SIZE)\n",
    "\n",
    "optimizer = AdamW(get_parameters_without_decay(model), lr=LEARNING_RATE, betas=(0.9, 0.999), \n",
    "                  eps =1e-6, weight_decay=0.01, correct_bias=True)\n",
    "lr_scheduler = WarmupLinearSchedule(optimizer, warmup_steps=0.1, \n",
    "                                    t_total=(len(corpus.train) / BATCH_SIZE)*MAX_N_EPOCHS)\n",
    "trainer = ModelTrainerBert(model=seq_tagger, \n",
    "                           optimizer=optimizer, \n",
    "                           lr_scheduler=lr_scheduler,\n",
    "                           train_dataloader=train_dataloader, \n",
    "                           val_dataloader=val_dataloader,\n",
    "                           update_scheduler='es',\n",
    "                           keep_best_model=True,\n",
    "                           restore_bm_on_lr_change=False,\n",
    "                           max_grad_norm=1.,\n",
    "                           validation_metrics=[f1_entity_level],\n",
    "                           decision_metric=lambda metrics: -metrics[1])\n",
    "\n",
    "trainer.train(epochs=MAX_N_EPOCHS)\n",
    "\n",
    "\n",
    "test_dataloader = create_loader_from_flair_corpus(corpus.test,\n",
    "                                                  SequentialSampler,\n",
    "                                                  batch_size=PRED_BATCH_SIZE)\n",
    "\n",
    "_, __, test_metrics = seq_tagger.predict(test_dataloader, evaluate=True, \n",
    "                                         metrics=[f1_entity_level, f1_token_level])\n",
    "logger.info(f'Entity-level f1: {test_metrics[1]}')\n",
    "logger.info(f'Token-level f1: {test_metrics[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-03 23:19:57,164 - sequence_tagger_bert - INFO - Train loss: 0.4787870369504153\n",
      "2019-11-03 23:20:00,145 - sequence_tagger_bert - INFO - Validation loss: 0.28253188729286194\n",
      "2019-11-03 23:20:00,146 - sequence_tagger_bert - INFO - Validation metrics: (0.3241469816272966,)\n",
      "2019-11-03 23:20:00,166 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   5%|▌         | 1/20 [00:56<17:50, 56.34s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-03 23:20:53,284 - sequence_tagger_bert - INFO - Train loss: 0.10299811927225305\n",
      "2019-11-03 23:20:56,507 - sequence_tagger_bert - INFO - Validation loss: 0.32408687472343445\n",
      "2019-11-03 23:20:56,508 - sequence_tagger_bert - INFO - Validation metrics: (0.4444444444444445,)\n",
      "2019-11-03 23:20:56,532 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  10%|█         | 2/20 [01:52<16:54, 56.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-03 23:21:50,103 - sequence_tagger_bert - INFO - Train loss: 0.0721234318756841\n",
      "2019-11-03 23:21:53,137 - sequence_tagger_bert - INFO - Validation loss: 0.36886006593704224\n",
      "2019-11-03 23:21:53,138 - sequence_tagger_bert - INFO - Validation metrics: (0.4554334554334554,)\n",
      "2019-11-03 23:21:53,166 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  15%|█▌        | 3/20 [02:49<15:59, 56.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-03 23:22:46,781 - sequence_tagger_bert - INFO - Train loss: 0.05845014256825719\n",
      "2019-11-03 23:22:49,799 - sequence_tagger_bert - INFO - Validation loss: 0.4011937975883484\n",
      "2019-11-03 23:22:49,800 - sequence_tagger_bert - INFO - Validation metrics: (0.4401215805471124,)\n",
      "2019-11-03 23:22:49,801 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  20%|██        | 4/20 [03:45<15:03, 56.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-03 23:23:43,306 - sequence_tagger_bert - INFO - Train loss: 0.051256966424883955\n",
      "2019-11-03 23:23:46,332 - sequence_tagger_bert - INFO - Validation loss: 0.4138947129249573\n",
      "2019-11-03 23:23:46,333 - sequence_tagger_bert - INFO - Validation metrics: (0.4666666666666667,)\n",
      "2019-11-03 23:23:46,352 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  25%|██▌       | 5/20 [04:42<14:07, 56.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-03 23:24:40,036 - sequence_tagger_bert - INFO - Train loss: 0.04522083744097868\n",
      "2019-11-03 23:24:43,204 - sequence_tagger_bert - INFO - Validation loss: 0.4281786382198334\n",
      "2019-11-03 23:24:43,205 - sequence_tagger_bert - INFO - Validation metrics: (0.46200980392156865,)\n",
      "2019-11-03 23:24:43,206 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  30%|███       | 6/20 [05:39<13:12, 56.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-03 23:25:37,547 - sequence_tagger_bert - INFO - Train loss: 0.04169752058896376\n",
      "2019-11-03 23:25:40,895 - sequence_tagger_bert - INFO - Validation loss: 0.40968552231788635\n",
      "2019-11-03 23:25:40,896 - sequence_tagger_bert - INFO - Validation metrics: (0.4748371817643576,)\n",
      "2019-11-03 23:25:40,916 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  35%|███▌      | 7/20 [06:37<12:20, 56.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-03 23:26:34,479 - sequence_tagger_bert - INFO - Train loss: 0.0381972702857049\n",
      "2019-11-03 23:26:37,556 - sequence_tagger_bert - INFO - Validation loss: 0.4394214153289795\n",
      "2019-11-03 23:26:37,557 - sequence_tagger_bert - INFO - Validation metrics: (0.47368421052631576,)\n",
      "2019-11-03 23:26:37,559 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  40%|████      | 8/20 [07:33<11:22, 56.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-03 23:27:31,507 - sequence_tagger_bert - INFO - Train loss: 0.035292348071662566\n",
      "2019-11-03 23:27:34,871 - sequence_tagger_bert - INFO - Validation loss: 0.42749372124671936\n",
      "2019-11-03 23:27:34,872 - sequence_tagger_bert - INFO - Validation metrics: (0.48184019370460046,)\n",
      "2019-11-03 23:27:34,891 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  45%|████▌     | 9/20 [08:31<10:26, 57.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-03 23:28:28,937 - sequence_tagger_bert - INFO - Train loss: 0.03331584910442329\n",
      "2019-11-03 23:28:31,938 - sequence_tagger_bert - INFO - Validation loss: 0.4554072320461273\n",
      "2019-11-03 23:28:31,939 - sequence_tagger_bert - INFO - Validation metrics: (0.4718137254901961,)\n",
      "2019-11-03 23:28:31,940 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  50%|█████     | 10/20 [09:28<09:30, 57.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-03 23:29:25,538 - sequence_tagger_bert - INFO - Train loss: 0.031775198278495065\n",
      "2019-11-03 23:29:28,536 - sequence_tagger_bert - INFO - Validation loss: 0.44853276014328003\n",
      "2019-11-03 23:29:28,537 - sequence_tagger_bert - INFO - Validation metrics: (0.48554216867469885,)\n",
      "2019-11-03 23:29:28,562 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  55%|█████▌    | 11/20 [10:24<08:32, 56.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-03 23:30:22,827 - sequence_tagger_bert - INFO - Train loss: 0.029974783775565537\n",
      "2019-11-03 23:30:25,918 - sequence_tagger_bert - INFO - Validation loss: 0.4577467739582062\n",
      "2019-11-03 23:30:25,919 - sequence_tagger_bert - INFO - Validation metrics: (0.4829683698296838,)\n",
      "2019-11-03 23:30:25,920 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  60%|██████    | 12/20 [11:22<07:36, 57.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-03 23:31:19,600 - sequence_tagger_bert - INFO - Train loss: 0.02827518145249761\n",
      "2019-11-03 23:31:22,733 - sequence_tagger_bert - INFO - Validation loss: 0.4715800881385803\n",
      "2019-11-03 23:31:22,734 - sequence_tagger_bert - INFO - Validation metrics: (0.48842874543239956,)\n",
      "2019-11-03 23:31:22,757 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  65%|██████▌   | 13/20 [12:18<06:38, 56.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-03 23:32:17,406 - sequence_tagger_bert - INFO - Train loss: 0.027177215616551707\n",
      "2019-11-03 23:32:20,518 - sequence_tagger_bert - INFO - Validation loss: 0.4620170593261719\n",
      "2019-11-03 23:32:20,519 - sequence_tagger_bert - INFO - Validation metrics: (0.49432835820895515,)\n",
      "2019-11-03 23:32:20,542 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  70%|███████   | 14/20 [13:16<05:43, 57.22s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-03 23:33:14,775 - sequence_tagger_bert - INFO - Train loss: 0.026193941682770155\n",
      "2019-11-03 23:33:17,778 - sequence_tagger_bert - INFO - Validation loss: 0.4734239876270294\n",
      "2019-11-03 23:33:17,779 - sequence_tagger_bert - INFO - Validation metrics: (0.4817518248175182,)\n",
      "2019-11-03 23:33:17,779 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  75%|███████▌  | 15/20 [14:13<04:46, 57.22s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-03 23:34:11,154 - sequence_tagger_bert - INFO - Train loss: 0.024578233642764197\n",
      "2019-11-03 23:34:14,366 - sequence_tagger_bert - INFO - Validation loss: 0.4755070209503174\n",
      "2019-11-03 23:34:14,367 - sequence_tagger_bert - INFO - Validation metrics: (0.49186256781193494,)\n",
      "2019-11-03 23:34:14,368 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  80%|████████  | 16/20 [15:10<03:48, 57.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-03 23:35:08,163 - sequence_tagger_bert - INFO - Train loss: 0.024713597943224593\n",
      "2019-11-03 23:35:11,260 - sequence_tagger_bert - INFO - Validation loss: 0.474751353263855\n",
      "2019-11-03 23:35:11,261 - sequence_tagger_bert - INFO - Validation metrics: (0.4940047961630696,)\n",
      "2019-11-03 23:35:11,262 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  85%|████████▌ | 17/20 [16:07<02:50, 56.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-03 23:36:04,699 - sequence_tagger_bert - INFO - Train loss: 0.024434842630120603\n",
      "2019-11-03 23:36:08,056 - sequence_tagger_bert - INFO - Validation loss: 0.48089444637298584\n",
      "2019-11-03 23:36:08,057 - sequence_tagger_bert - INFO - Validation metrics: (0.4933973589435774,)\n",
      "2019-11-03 23:36:08,058 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  90%|█████████ | 18/20 [17:04<01:53, 56.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-03 23:37:01,138 - sequence_tagger_bert - INFO - Train loss: 0.023629867640647246\n",
      "2019-11-03 23:37:04,316 - sequence_tagger_bert - INFO - Validation loss: 0.477573037147522\n",
      "2019-11-03 23:37:04,317 - sequence_tagger_bert - INFO - Validation metrics: (0.4924924924924925,)\n",
      "2019-11-03 23:37:04,318 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  95%|█████████▌| 19/20 [18:00<00:56, 56.73s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-03 23:37:58,003 - sequence_tagger_bert - INFO - Train loss: 0.023345020170193263\n",
      "2019-11-03 23:38:01,070 - sequence_tagger_bert - INFO - Validation loss: 0.4798295199871063\n",
      "2019-11-03 23:38:01,071 - sequence_tagger_bert - INFO - Validation metrics: (0.4924924924924925,)\n",
      "2019-11-03 23:38:01,072 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 100%|██████████| 20/20 [18:57<00:00, 56.74s/it]\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-03 23:38:04,720 - sequence_tagger_bert - INFO - Entity-level f1: 0.5571428571428572\n",
      "2019-11-03 23:38:04,721 - sequence_tagger_bert - INFO - Token-level f1: 0.5844269466316709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-03 23:38:58,046 - sequence_tagger_bert - INFO - Train loss: 0.025655677117445926\n",
      "2019-11-03 23:39:01,038 - sequence_tagger_bert - INFO - Validation loss: 0.4682237505912781\n",
      "2019-11-03 23:39:01,039 - sequence_tagger_bert - INFO - Validation metrics: (0.4930681133212779,)\n",
      "2019-11-03 23:39:01,065 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   5%|▌         | 1/20 [00:56<17:48, 56.26s/it]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e567414ac06d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m                                    decision_metric=lambda metrics: -metrics[1])\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_N_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/bert_sequence_tagger/src/bert_sequence_tagger/model_trainer_bert.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     64\u001b[0m                                                    max_norm=self._max_grad_norm)\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bert_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pytorch_transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    174\u001b[0m                     \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0;31m# Just adding the square of the weights to the loss function is *not*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#BATCH_SIZE = 10\n",
    "#PRED_BATCH_SIZE = 10\n",
    "#MAX_N_EPOCHS = 20\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "\n",
    "from bert_sequence_tagger.bert_utils import create_loader_from_flair_corpus, get_parameters_without_decay\n",
    "from bert_sequence_tagger.model_trainer_bert import ModelTrainerBert\n",
    "\n",
    "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
    "\n",
    "from bert_sequence_tagger.metrics import f1_entity_level, f1_token_level\n",
    "\n",
    "for LEARNING_RATE in [3e-6, 3e-7, 3e-4, 1e-5]:\n",
    "\n",
    "        train_dataloader = create_loader_from_flair_corpus(corpus.train, \n",
    "                                                           RandomSampler, \n",
    "                                                           batch_size=BATCH_SIZE)\n",
    "        val_dataloader = create_loader_from_flair_corpus(corpus.dev,\n",
    "                                                         SequentialSampler,\n",
    "                                                         batch_size=PRED_BATCH_SIZE)\n",
    "\n",
    "        optimizer = AdamW(get_parameters_without_decay(model), lr=LEARNING_RATE, betas=(0.9, 0.999), \n",
    "                          eps =1e-6, weight_decay=0.01, correct_bias=True)\n",
    "        lr_scheduler = WarmupLinearSchedule(optimizer, warmup_steps=0.1, \n",
    "                                            t_total=(len(corpus.train) / BATCH_SIZE)*MAX_N_EPOCHS)\n",
    "        trainer = ModelTrainerBert(model=seq_tagger, \n",
    "                                   optimizer=optimizer, \n",
    "                                   lr_scheduler=lr_scheduler,\n",
    "                                   train_dataloader=train_dataloader, \n",
    "                                   val_dataloader=val_dataloader,\n",
    "                                   update_scheduler='es',\n",
    "                                   keep_best_model=True,\n",
    "                                   restore_bm_on_lr_change=False,\n",
    "                                   max_grad_norm=1.,\n",
    "                                   validation_metrics=[f1_entity_level],\n",
    "                                   decision_metric=lambda metrics: -metrics[1])\n",
    "\n",
    "        trainer.train(epochs=MAX_N_EPOCHS)\n",
    "\n",
    "\n",
    "        test_dataloader = create_loader_from_flair_corpus(corpus.test,\n",
    "                                                          SequentialSampler,\n",
    "                                                          batch_size=PRED_BATCH_SIZE)\n",
    "\n",
    "        _, __, test_metrics = seq_tagger.predict(test_dataloader, evaluate=True, \n",
    "                                                 metrics=[f1_entity_level, f1_token_level])\n",
    "        logger.info(f'Entity-level f1: {test_metrics[1]}')\n",
    "        logger.info(f'Token-level f1: {test_metrics[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('labels_v3_lr7.txt', 'w') as f:\n",
    "    for _string in _:\n",
    "        #f.seek(0)\n",
    "        f.write(', '.join(_string) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## отвалидировалась на существующих метриках, лучше при lr 3e-6, 3e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-23 17:36:07,138 - sequence_tagger_bert - INFO - Train loss: 0.44207449675415433\n",
      "2019-10-23 17:36:09,913 - sequence_tagger_bert - INFO - Validation loss: 0.19390782713890076\n",
      "2019-10-23 17:36:09,914 - sequence_tagger_bert - INFO - Validation metrics: (0.5417457305502846,)\n",
      "2019-10-23 17:36:09,933 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   5%|▌         | 1/20 [00:49<15:37, 49.32s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-23 17:37:00,264 - sequence_tagger_bert - INFO - Train loss: 0.09446393226471422\n",
      "2019-10-23 17:37:03,435 - sequence_tagger_bert - INFO - Validation loss: 0.2510414719581604\n",
      "2019-10-23 17:37:03,438 - sequence_tagger_bert - INFO - Validation metrics: (0.389370306181398,)\n",
      "2019-10-23 17:37:03,438 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  10%|█         | 2/20 [01:42<15:10, 50.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-23 17:37:53,917 - sequence_tagger_bert - INFO - Train loss: 0.06731621946347245\n",
      "2019-10-23 17:37:56,680 - sequence_tagger_bert - INFO - Validation loss: 0.26085543632507324\n",
      "2019-10-23 17:37:56,683 - sequence_tagger_bert - INFO - Validation metrics: (0.4366041896361632,)\n",
      "2019-10-23 17:37:56,685 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  15%|█▌        | 3/20 [02:36<14:33, 51.38s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-23 17:38:44,783 - sequence_tagger_bert - INFO - Train loss: 0.055439955626542754\n",
      "2019-10-23 17:38:47,742 - sequence_tagger_bert - INFO - Validation loss: 0.27321499586105347\n",
      "2019-10-23 17:38:47,749 - sequence_tagger_bert - INFO - Validation metrics: (0.44395361678630596,)\n",
      "2019-10-23 17:38:47,750 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  20%|██        | 4/20 [03:27<13:40, 51.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-23 17:39:40,312 - sequence_tagger_bert - INFO - Train loss: 0.048552938294491615\n",
      "2019-10-23 17:39:43,413 - sequence_tagger_bert - INFO - Validation loss: 0.2877540588378906\n",
      "2019-10-23 17:39:43,414 - sequence_tagger_bert - INFO - Validation metrics: (0.45419637959407566,)\n",
      "2019-10-23 17:39:43,414 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  25%|██▌       | 5/20 [04:22<13:08, 52.60s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-23 17:40:31,209 - sequence_tagger_bert - INFO - Train loss: 0.042401829072826316\n",
      "2019-10-23 17:40:33,981 - sequence_tagger_bert - INFO - Validation loss: 0.2951381504535675\n",
      "2019-10-23 17:40:33,982 - sequence_tagger_bert - INFO - Validation metrics: (0.4681528662420382,)\n",
      "2019-10-23 17:40:33,982 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  30%|███       | 6/20 [05:13<12:07, 51.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-23 17:41:23,348 - sequence_tagger_bert - INFO - Train loss: 0.039143779795465156\n",
      "2019-10-23 17:41:26,505 - sequence_tagger_bert - INFO - Validation loss: 0.3359852731227875\n",
      "2019-10-23 17:41:26,506 - sequence_tagger_bert - INFO - Validation metrics: (0.4252232142857143,)\n",
      "2019-10-23 17:41:26,511 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  35%|███▌      | 7/20 [06:05<11:17, 52.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-23 17:42:17,811 - sequence_tagger_bert - INFO - Train loss: 0.03634464747917194\n",
      "2019-10-23 17:42:20,560 - sequence_tagger_bert - INFO - Validation loss: 0.3336271643638611\n",
      "2019-10-23 17:42:20,561 - sequence_tagger_bert - INFO - Validation metrics: (0.45098039215686275,)\n",
      "2019-10-23 17:42:20,561 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  40%|████      | 8/20 [06:59<10:32, 52.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-23 17:43:08,055 - sequence_tagger_bert - INFO - Train loss: 0.03377418829195572\n",
      "2019-10-23 17:43:10,806 - sequence_tagger_bert - INFO - Validation loss: 0.34779462218284607\n",
      "2019-10-23 17:43:10,808 - sequence_tagger_bert - INFO - Validation metrics: (0.42690383546414673,)\n",
      "2019-10-23 17:43:10,809 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  45%|████▌     | 9/20 [07:50<09:31, 51.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-23 17:44:01,690 - sequence_tagger_bert - INFO - Train loss: 0.03119575367962343\n",
      "2019-10-23 17:44:04,794 - sequence_tagger_bert - INFO - Validation loss: 0.3810552656650543\n",
      "2019-10-23 17:44:04,795 - sequence_tagger_bert - INFO - Validation metrics: (0.40594625500285875,)\n",
      "2019-10-23 17:44:04,796 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  50%|█████     | 10/20 [08:44<08:45, 52.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-23 17:44:55,029 - sequence_tagger_bert - INFO - Train loss: 0.02917186085480303\n",
      "2019-10-23 17:44:57,722 - sequence_tagger_bert - INFO - Validation loss: 0.3724142014980316\n",
      "2019-10-23 17:44:57,723 - sequence_tagger_bert - INFO - Validation metrics: (0.44114411441144114,)\n",
      "2019-10-23 17:44:57,724 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  55%|█████▌    | 11/20 [09:37<07:54, 52.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-23 17:45:46,541 - sequence_tagger_bert - INFO - Train loss: 0.027264553147507198\n",
      "2019-10-23 17:45:49,428 - sequence_tagger_bert - INFO - Validation loss: 0.385182648897171\n",
      "2019-10-23 17:45:49,429 - sequence_tagger_bert - INFO - Validation metrics: (0.42333333333333334,)\n",
      "2019-10-23 17:45:49,429 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  60%|██████    | 12/20 [10:28<06:59, 52.39s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-23 17:46:41,505 - sequence_tagger_bert - INFO - Train loss: 0.026822505367085404\n",
      "2019-10-23 17:46:44,564 - sequence_tagger_bert - INFO - Validation loss: 0.3787497580051422\n",
      "2019-10-23 17:46:44,566 - sequence_tagger_bert - INFO - Validation metrics: (0.4508419337316676,)\n",
      "2019-10-23 17:46:44,569 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  65%|██████▌   | 13/20 [11:23<06:12, 53.22s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-23 17:47:33,097 - sequence_tagger_bert - INFO - Train loss: 0.02519219604476747\n",
      "2019-10-23 17:47:36,121 - sequence_tagger_bert - INFO - Validation loss: 0.39794737100601196\n",
      "2019-10-23 17:47:36,122 - sequence_tagger_bert - INFO - Validation metrics: (0.43468715697036225,)\n",
      "2019-10-23 17:47:36,123 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  70%|███████   | 14/20 [12:15<05:16, 52.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-23 17:48:29,517 - sequence_tagger_bert - INFO - Train loss: 0.024216730564798974\n",
      "2019-10-23 17:48:32,981 - sequence_tagger_bert - INFO - Validation loss: 0.39265915751457214\n",
      "2019-10-23 17:48:32,982 - sequence_tagger_bert - INFO - Validation metrics: (0.44782608695652176,)\n",
      "2019-10-23 17:48:32,983 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  75%|███████▌  | 15/20 [13:12<04:29, 53.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-23 17:49:23,983 - sequence_tagger_bert - INFO - Train loss: 0.02380452183237493\n",
      "2019-10-23 17:49:26,647 - sequence_tagger_bert - INFO - Validation loss: 0.4019433856010437\n",
      "2019-10-23 17:49:26,648 - sequence_tagger_bert - INFO - Validation metrics: (0.44480874316939895,)\n",
      "2019-10-23 17:49:26,649 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  80%|████████  | 16/20 [14:06<03:35, 53.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-23 17:50:14,958 - sequence_tagger_bert - INFO - Train loss: 0.022866339103015335\n",
      "2019-10-23 17:50:17,961 - sequence_tagger_bert - INFO - Validation loss: 0.4048379957675934\n",
      "2019-10-23 17:50:17,962 - sequence_tagger_bert - INFO - Validation metrics: (0.4456521739130435,)\n",
      "2019-10-23 17:50:17,963 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  85%|████████▌ | 17/20 [14:57<02:39, 53.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-23 17:51:09,676 - sequence_tagger_bert - INFO - Train loss: 0.022506936902087604\n",
      "2019-10-23 17:51:12,574 - sequence_tagger_bert - INFO - Validation loss: 0.4067346751689911\n",
      "2019-10-23 17:51:12,575 - sequence_tagger_bert - INFO - Validation metrics: (0.4397163120567376,)\n",
      "2019-10-23 17:51:12,577 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  90%|█████████ | 18/20 [15:51<01:47, 53.56s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-23 17:52:00,266 - sequence_tagger_bert - INFO - Train loss: 0.022491128675900904\n",
      "2019-10-23 17:52:02,964 - sequence_tagger_bert - INFO - Validation loss: 0.4055901765823364\n",
      "2019-10-23 17:52:02,965 - sequence_tagger_bert - INFO - Validation metrics: (0.44189852700491,)\n",
      "2019-10-23 17:52:02,966 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  95%|█████████▌| 19/20 [16:42<00:52, 52.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-23 17:52:52,142 - sequence_tagger_bert - INFO - Train loss: 0.022022977412353813\n",
      "2019-10-23 17:52:55,384 - sequence_tagger_bert - INFO - Validation loss: 0.4082149267196655\n",
      "2019-10-23 17:52:55,386 - sequence_tagger_bert - INFO - Validation metrics: (0.4378762999452655,)\n",
      "2019-10-23 17:52:55,387 - sequence_tagger_bert - INFO - Current learning rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 100%|██████████| 20/20 [17:34<00:00, 52.55s/it]\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-23 17:52:59,515 - sequence_tagger_bert - INFO - Entity-level f1: 0.5282833251352681\n",
      "2019-10-23 17:52:59,516 - sequence_tagger_bert - INFO - Token-level f1: 0.4879333040807372\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "\n",
    "from bert_sequence_tagger.bert_utils import create_loader_from_flair_corpus, get_parameters_without_decay\n",
    "from bert_sequence_tagger.model_trainer_bert import ModelTrainerBert\n",
    "\n",
    "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
    "\n",
    "from bert_sequence_tagger.metrics import f1_entity_level, f1_token_level\n",
    "\n",
    "train_dataloader = create_loader_from_flair_corpus(corpus.train, \n",
    "                                                           RandomSampler, \n",
    "                                                           batch_size=BATCH_SIZE)\n",
    "val_dataloader = create_loader_from_flair_corpus(corpus.dev,\n",
    "                                                 SequentialSampler,\n",
    "                                                 batch_size=PRED_BATCH_SIZE)\n",
    "\n",
    "optimizer = AdamW(get_parameters_without_decay(model), lr=LEARNING_RATE, betas=(0.9, 0.999), \n",
    "                  eps =1e-6, weight_decay=0.01, correct_bias=True)\n",
    "lr_scheduler = WarmupLinearSchedule(optimizer, warmup_steps=0.1, \n",
    "                                    t_total=(len(corpus.train) / BATCH_SIZE)*MAX_N_EPOCHS)\n",
    "trainer = ModelTrainerBert(model=seq_tagger, \n",
    "                           optimizer=optimizer, \n",
    "                           lr_scheduler=lr_scheduler,\n",
    "                           train_dataloader=train_dataloader, \n",
    "                           val_dataloader=val_dataloader,\n",
    "                           update_scheduler='es',\n",
    "                           keep_best_model=True,\n",
    "                           restore_bm_on_lr_change=False,\n",
    "                           max_grad_norm=1.,\n",
    "                           validation_metrics=[f1_entity_level],\n",
    "                           decision_metric=lambda metrics: -metrics[1])\n",
    "\n",
    "trainer.train(epochs=MAX_N_EPOCHS)\n",
    "\n",
    "\n",
    "test_dataloader = create_loader_from_flair_corpus(corpus.test,\n",
    "                                                  SequentialSampler,\n",
    "                                                  batch_size=PRED_BATCH_SIZE)\n",
    "\n",
    "preds, probs, test_metrics = seq_tagger.predict(test_dataloader, evaluate=True, \n",
    "                                         metrics=[f1_entity_level, f1_token_level])\n",
    "logger.info(f'Entity-level f1: {test_metrics[1]}')\n",
    "logger.info(f'Token-level f1: {test_metrics[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "', '.join(preds[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('labels_v.txt', 'w') as f:\n",
    "    for _string in preds:\n",
    "        #f.seek(0)\n",
    "        f.write(', '.join(_string) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_flair_corpus(corpus, name='tag', filter_tokens={'-DOCSTART-'}):\n",
    "    result = []\n",
    "    for sent in corpus[:10]:\n",
    "        print (\"sent\", sent)\n",
    "        print (\"sent[0].text\", sent[0].text)\n",
    "        if sent[0].text in filter_tokens:\n",
    "            continue\n",
    "        else:\n",
    "            result.append(([token.text for token in sent.tokens],\n",
    "                           [token.tags[name].value for token in sent.tokens]))\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-23 12:13:32,709 - sequence_tagger_bert - INFO - Entity-level f1: 0.43243243243243246\n",
      "2019-10-23 12:13:32,711 - sequence_tagger_bert - INFO - Token-level f1: 0.43655723158828746\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = create_loader_from_flair_corpus(corpus.test,\n",
    "                                                  SequentialSampler,\n",
    "                                                  batch_size=PRED_BATCH_SIZE)\n",
    "\n",
    "_, __, test_metrics = seq_tagger.predict(test_dataloader, evaluate=True, \n",
    "                                         metrics=[f1_entity_level, f1_token_level])\n",
    "logger.info(f'Entity-level f1: {test_metrics[1]}')\n",
    "logger.info(f'Token-level f1: {test_metrics[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.9143007822800387, 0.9306361914074436)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluatorBase():\n",
    "    \"\"\"EvaluatorBase is abstract base class for all evaluators\"\"\"\n",
    "    def get_evaluation_score_train_dev_test(self, tagger, datasets_bank, batch_size=-1):\n",
    "        if batch_size == -1:\n",
    "            batch_size = tagger.batch_size\n",
    "        score_train, _ = self.predict_evaluation_score(tagger=tagger,\n",
    "                                                       word_sequences=datasets_bank.word_sequences_train,\n",
    "                                                       targets_tag_sequences=datasets_bank.tag_sequences_train,\n",
    "                                                       batch_size=batch_size)\n",
    "        score_dev, _ = self.predict_evaluation_score(tagger=tagger,\n",
    "                                                     word_sequences=datasets_bank.word_sequences_dev,\n",
    "                                                     targets_tag_sequences=datasets_bank.tag_sequences_dev,\n",
    "                                                     batch_size=batch_size)\n",
    "        score_test, msg_test = self.predict_evaluation_score(tagger=tagger,\n",
    "                                                             word_sequences=datasets_bank.word_sequences_test,\n",
    "                                                             targets_tag_sequences=datasets_bank.tag_sequences_test,\n",
    "                                                             batch_size=batch_size)\n",
    "        return score_train, score_dev, score_test, msg_test\n",
    "\n",
    "    def predict_evaluation_score(self, tagger, word_sequences, targets_tag_sequences, batch_size):\n",
    "        outputs_tag_sequences = tagger.predict_tags_from_words(word_sequences, batch_size)\n",
    "        return self.get_evaluation_score(targets_tag_sequences, outputs_tag_sequences, word_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluatorF1MacroTokenLevel(EvaluatorBase):\n",
    "    def __init__(self):\n",
    "        self.tag_list = None\n",
    "        self.tag2idx = dict()\n",
    "\n",
    "    def __init_tag_list(self, targets_tag_sequences):\n",
    "        if self.tag_list is not None:\n",
    "            return\n",
    "        self.tag_list = list()\n",
    "        for tag_seq in targets_tag_sequences:\n",
    "            for t in tag_seq:\n",
    "                if t not in self.tag_list:\n",
    "                    self.tag_list.append(t)\n",
    "                    self.tag2idx[t] = len(self.tag_list)\n",
    "        self.tag_list.sort()\n",
    "\n",
    "    def tag_seq_2_idx_list(self, tag_seq):\n",
    "        return [self.tag2idx[t] for t in tag_seq]\n",
    "\n",
    "    def __get_zeros_tag_dict(self):\n",
    "        return {tag: 0 for tag in self.tag_list}\n",
    "\n",
    "    def __add_dict(self, dict1, dict2):\n",
    "        for tag in self.tag_list:\n",
    "            dict1[tag] += dict2[tag]\n",
    "        return dict1\n",
    "\n",
    "    def __div_dict(self, dict, d):\n",
    "        for tag in self.tag_list:\n",
    "            dict[tag] /= d\n",
    "        return dict\n",
    "\n",
    "    def __get_M_F1_msg(self, F1, precision, recall):\n",
    "        msg = '\\nF1 scores\\n'\n",
    "        msg += '-' * 24 + '\\n'\n",
    "        sum_M_F1 = 0\n",
    "        sum_precision = 0\n",
    "        sum_recall = 0\n",
    "        for tag in self.tag_list:\n",
    "            sum_M_F1 += F1[tag]\n",
    "            sum_precision += precision[tag]\n",
    "            sum_recall += recall[tag]\n",
    "            msg += '%15s = f1 = %1.2f, precision = %1.2f, recall = %1.2f\\n' % (tag, F1[tag], precision[tag], recall[tag])\n",
    "        M_F1 = sum_M_F1 / len(F1)\n",
    "        M_PR = sum_precision / len(F1)\n",
    "        M_RE = sum_recall / len(F1)\n",
    "        msg += '-'*24 + '\\n'\n",
    "        msg += 'Macro-F1 = %1.3f' % M_F1\n",
    "        msg += 'Macro-Prescion = %1.3f' % M_PR\n",
    "        msg += 'Macro-Recall = %1.3f' % M_RE\n",
    "        return M_F1, msg\n",
    "\n",
    "    def __add_to_dict(self, dict_in, tag, val):\n",
    "        if tag in dict_in:\n",
    "            dict_in[tag] += val\n",
    "        else:\n",
    "            dict_in[tag] = val\n",
    "        return dict_in\n",
    "\n",
    "    \"\"\"EvaluatorF1MacroTagComponents is macro-F1 scores evaluator for each class of BOI-like tags.\"\"\"\n",
    "    def get_evaluation_score(self, targets_tag_sequences, outputs_tag_sequences, word_sequences=None):\n",
    "        # Create list of tags\n",
    "        self.__init_tag_list(targets_tag_sequences)\n",
    "        i = 0\n",
    "        for elem in zip(targets_tag_sequences, outputs_tag_sequences):\n",
    "            if (i < 4):\n",
    "                i = i +1\n",
    "                print (elem[0])\n",
    "                print (elem[1])\n",
    "        # Init values\n",
    "        TP = self.__get_zeros_tag_dict()\n",
    "        FP = self.__get_zeros_tag_dict()\n",
    "        FN = self.__get_zeros_tag_dict()\n",
    "        F1 = self.__get_zeros_tag_dict()\n",
    "        precision = self.__get_zeros_tag_dict()\n",
    "        recall = self.__get_zeros_tag_dict()\n",
    "        for targets_seq, outputs_tag_seq in zip(targets_tag_sequences, outputs_tag_sequences):\n",
    "            for t, o in zip(targets_seq, outputs_tag_seq):\n",
    "                if t == o:\n",
    "                    TP = self.__add_to_dict(TP, t, 1)\n",
    "                else:\n",
    "                    FN = self.__add_to_dict(FN, t, 1)\n",
    "                    FP = self.__add_to_dict(FP, o, 1)\n",
    "        # Calculate F1 for each tag\n",
    "        for tag in self.tag_list:\n",
    "            F1[tag] = (2 * TP[tag] / max(2 * TP[tag] + FP[tag] + FN[tag], 1)) * 100\n",
    "            precision[tag] = (TP[tag] / max(TP[tag] + FP[tag], 1))*100\n",
    "            recall[tag] = (TP[tag] / max(TP[tag] + FN[tag], 1))*100\n",
    "        # Calculate Macro-F1 score and prepare the message\n",
    "        M_F1, msg = self.__get_M_F1_msg(F1,precision, recall)\n",
    "        print(msg)\n",
    "        #self.validate_M_F1_scikitlearn( targets_tag_sequences, outputs_tag_sequences)\n",
    "        return M_F1, msg\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
